{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT on News Memorization\n",
    "\n",
    "This file is used to train a supervised fine-tuned model for the news memorization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 01:42:45.937930: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-31 01:42:46.139543: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-31 01:42:46.139580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-31 01:42:46.176912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-31 01:42:46.257761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-31 01:42:47.001093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the huggingface key\n",
    "import json\n",
    "with open('../apikeys.json', 'r') as f:\n",
    "    apikeys = json.load(f)\n",
    "hf_key = apikeys['hf_api_key']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Config for SFT Training\n",
    "\n",
    "Before running the training, we need to setup the config for the SFT training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import DPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT General Configs\n",
    "sft_model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "sft_data_dir = \"../datasets/latest_news/latest_news_memorization.csv\"\n",
    "sft_output_dir = \"./sft_models/latest_news_memorization\"\n",
    "sft_log_dir = \"./sft_logs/latest_news_memorization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb Configs\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft Configs (Lora Config)\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,  # batch size per device\n",
    "    gradient_accumulation_steps=2,  # number of updates steps to accumulate before performing a backward/update pass\n",
    "    gradient_checkpointing =False,  # disable gradient checkpointing\n",
    "    max_grad_norm= 0.3,  # max gradient norm\n",
    "    num_train_epochs=100,  # number of training epochs\n",
    "    save_steps= 100,  # save the model every x steps (step = batch size * gradient accumulation steps)\n",
    "    learning_rate=2e-4,  # learning rate\n",
    "    bf16=True,  # use bf16 for training\n",
    "    save_total_limit=2,  # save the best 2 checkpoints (1 best and 1 last)\n",
    "    evaluation_strategy=\"epoch\",  # evaluate the model every x epochs\n",
    "    output_dir=sft_output_dir,  # output directory\n",
    "    logging_dir=sft_log_dir,  # logging directory\n",
    "    optim=\"paged_adamw_32bit\",  # optimizer\n",
    "    lr_scheduler_type=\"cosine\",  # learning rate scheduler type\n",
    "    warmup_ratio=0.05,  # warmup ratio\n",
    "    remove_unused_columns=False  # remove unused columns\n",
    ")\n",
    "\n",
    "generate_max_length = 64\n",
    "tokenizer_max_length = 512  # max length for tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Then we need to load the dataset. Specifically, the dataaset is a csv file with the following columns: `id`, `prompt`, `answer`, `article_title`, `question`, `fact`, `article_text`, `used_in_analysis`. For sft learning, we only need the `prompt` and `answer` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_dataset():\n",
    "    dataset = pd.read_csv(sft_data_dir)\n",
    "    dataset = Dataset.from_pandas(dataset)  # convert to huggingface dataset\n",
    "    tokenizer = AutoTokenizer.from_pretrained(sft_model_name, token=hf_key)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # set pad token to eos token\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"prompt\"] + examples[\"answer\"], \n",
    "                         padding=\"max_length\", \n",
    "                         max_length=tokenizer_max_length,\n",
    "                         truncation=True,\n",
    "                         return_tensors=\"pt\",\n",
    "                         )\n",
    "        \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"id\", \"prompt\", \"answer\", \"article_title\", \"question\", \"fact\", \"article_text\", \"used_in_analysis\"])    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_dataset():\n",
    "    dataset = pd.read_csv(sft_data_dir)\n",
    "    dataset = Dataset.from_pandas(dataset)  # convert to huggingface dataset\n",
    "    # remove all the columns except for `prompt`, `answer`\n",
    "    dataset = dataset.remove_columns([\"id\", \"article_title\", \"question\", \"fact\", \"article_text\", \"used_in_analysis\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Then we need to load the model. We use the `meta-llama/Meta-Llama-3-8B` model as the base model to be fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = AutoModelForCausalLM.from_pretrained(sft_model_name, \n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 device_map=\"auto\",  # use auto device mapping (GPU)\n",
    "                                                 token=hf_key,\n",
    "                                                 )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(sft_model_name, token=hf_key)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Accuracy)\n",
    "\n",
    "We need to evaluate the model's accuracy on the dataset. Specifically, we need to check if the model's answer is the same as the ground truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(eval_dataset, model, tokenizer):\n",
    "    correct = 0\n",
    "    total = len(eval_dataset)\n",
    "    for example in eval_dataset:\n",
    "        prompt = example[\"prompt\"]\n",
    "        oracle_answer = example[\"answer\"]\n",
    "\n",
    "        # Generate model output\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids\n",
    "        output_ids = model.generate(input_ids, max_length=64)\n",
    "        generated_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Compare generated answer with oracle\n",
    "        if generated_answer.strip().lower() == oracle_answer.strip().lower():\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(epochs, accuracies):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, accuracies, label='Accuracy', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('News Memorization Accuracy v.s. Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{sft_output_dir}/accuracy.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model, tokenizer = load_model()\n",
    "    train_dataset = load_train_dataset()\n",
    "    eval_dataset = load_eval_dataset()\n",
    "    model = get_peft_model(model, peft_config)  # apply peft to the model to add LoRA layers\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=tokenizer_max_length,\n",
    "    )\n",
    "    \n",
    "    epochs = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # Training and evaluation loop\n",
    "    for epoch in range(int(training_args.num_train_epochs)):\n",
    "        print(f\"Epoch {epoch + 1} / {training_args.num_train_epochs}\")\n",
    "        trainer.train()  # Train for one epoch\n",
    "        accuracy = evaluate_accuracy(eval_dataset, model, tokenizer)  # Evaluate accuracy\n",
    "        epochs.append(epoch + 1)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Accuracy after epoch {epoch + 1}: {accuracy:.4f}\")\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(sft_output_dir)\n",
    "    plot_accuracy(epochs, accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
